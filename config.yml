# Import framework configuration
import: src/framework/config.yml

# Build directory (use absolute path to avoid compose path resolution issues)
build_dir: ${PROJECT_ROOT}/build

# Root directory (absolute path)
# If you operate on local and remote machines, recommended to use environment variable
project_root: ${PROJECT_ROOT}

# Application Configuration (paths are derived by convention)
# - Config: src/applications/{app}/config.yml
# - Registry: src/applications/{app}/registry.py

applications:
  - otter
  # - als_assistant
  # - wind_turbine
  # - hello_world_weather

# ============================================================
# MODEL CONFIGURATION OVERRIDES - Use Stanford AI Playground
# ============================================================
# Override framework model defaults to use Stanford AI Playground
# Available Stanford models: claude-3-7-sonnet, gpt-4o, gpt-4.omini,
# deepseek-r1, gemini-2.0-flash-001, o3-mini

framework:
  models:
    orchestrator:
      provider: stanford
      model_id: claude-3-7-sonnet  # Replaces cborg's anthropic/claude-sonnet
    response:
      provider: stanford
      model_id: gpt-4o  # Replaces cborg's google/gemini-flash
      max_tokens: 5000
    classifier:
      provider: stanford
      model_id: gpt-4.omini  # Replaces ollama's mistral:7b
    approval:
      provider: stanford
      model_id: gpt-4.omini  # Replaces ollama's mistral:7b
    task_extraction:
      provider: stanford
      model_id: gpt-4o  # Replaces cborg's google/gemini-flash
      max_tokens: 1024
    memory:
      provider: stanford
      model_id: gpt-4o  # Replaces cborg's google/gemini-flash
      max_tokens: 256
    python_code_generator:
      provider: stanford
      model_id: claude-3-7-sonnet  # Replaces cborg's anthropic/claude-haiku
      max_tokens: 4096
    time_parsing:
      provider: stanford
      model_id: gpt-4.omini  # Replaces ollama's mistral:7b
      max_tokens: 512
    filter_extraction:
      provider: stanford
      model_id: gpt-4.omini  # Fast model for extracting run query filters
      max_tokens: 1024  # More tokens than time_parsing for complex filter combinations

# ============================================================
# OPERATOR CONTROLS - System-wide governance and safety policies
# ============================================================

# Approval Management - System-wide safety policy
approval:
  global_mode: "selective"  # "disabled" | "selective" | "all_capabilities"
  capabilities:
    python_execution:
      enabled: true
      mode: "epics_writes"      # "disabled" | "epics_writes" | "all_code"
    memory:
      enabled: true          # Simple on/off for memory operations

# Execution Control - System-wide governance limits and safety controls
execution_control:
  epics:
    writes_enabled: false            # Master switch: can EPICS writes ever happen?

  # Execution safety limits and control flow
  limits:
    max_reclassifications: 1         # Maximum times a task can be reclassified
    max_planning_attempts: 2         # Maximum number of planning attempts before giving up
    max_step_retries: 3              # Maximum retries per step before failing
    max_execution_time_seconds: 3000 # Maximum execution time (50 minutes)
    graph_recursion_limit: 100       # LangGraph recursion limit to prevent infinite loops

# System Configuration - Infrastructure-wide settings
system:
  # Timezone configuration for all containers and services
  # Ensures consistent timestamps across all framework components
  timezone: ${TZ:-America/Los_Angeles}  # Use host TZ env var or default to Pacific Time

# File Paths - System-wide data organization (operators control where data goes)
file_paths:
  # Parent directory for all agent-related data
  agent_data_dir: _agent_data
  # Subdirectories within agent_data_dir
  executed_python_scripts_dir: executed_scripts
  execution_plans_dir: execution_plans
  user_memory_dir: user_memory
  registry_exports_dir: registry_exports
  prompts_dir: prompts
  checkpoints: checkpoints

# ============================================================
# Service Deployment Control (Override Layer)
# ============================================================
# This section overrides the deployed_services from framework and application configs.
# It provides a convenient single place to control which containers are started when you run:
# python deployment/container_manager.py config.yml up
#
# IMPORTANT: This does not disable functionality - it only controls which
# containers get started. Services not listed here simply won't have their
# containers launched, but their configurations remain available.
#
# Each imported config (framework, applications) has its own deployed_services section,
# but this main config acts as the final override for convenience.
#
# You can use either full service keys (framework.jupyter) or short names (jupyter).
# Full keys are recommended for clarity about what you're starting.

deployed_services:
  # Framework services (essential for framework operation, see also src/framework/config.yml)
  - framework.jupyter          # Python execution environment with EPICS support
  - framework.open_webui       # Primary user interface frontend
  - framework.pipelines        # OpenWebUI pipeline interface server

  # ALS Assistant application services (domain-specific) - MOVED HERE FROM src/applications/als_assistant/config.yml as CONVENIENCE
  # - applications.als_assistant.mongo        # Primary database for ALS Assistant PV data
  # - applications.als_assistant.pv_finder    # PV discovery for EPICS - standalone MCP server
  # - applications.als_assistant.langfuse     # Observability and tracing infrastructure


# Development Configuration
development:
  # If true, exceptions will be raised directly instead of being wrapped in ExecutionError
  # This provides full stack traces for debugging but should be false in production
  raise_raw_errors: false

  # Prompt debugging configuration
  prompts:
    # If true, prints all system prompts to console when they are built
    # Useful for debugging prompt generation and seeing what's being sent to LLMs
    show_all: false

    # If true, saves all system prompts to files for inspection
    # Prompts are saved to the prompts_dir specified in agent configuration
    print_all: true

    # If true, only keeps the latest version of each prompt file
    # If false, saves prompts with timestamps to track changes over time
    latest_only: true

# Logging Configuration
logging:
  # Rich logging traceback settings
  rich_tracebacks: false              # Enable/disable rich-formatted tracebacks
  show_traceback_locals: false       # Show local variables in tracebacks
  show_full_paths: false             # Show full file paths in tracebacks (vs relative paths)


# Global API Provider Configuration (shared across all applications)
api:
  providers:
    cborg:
      api_key: ${CBORG_API_KEY}
      base_url: https://api.cborg.lbl.gov/v1 #https://api-local.cborg.lbl.gov/v1
      timeout: 30
    stanford:
      api_key: ${STANFORD_API_KEY}
      base_url: https://aiapi-prod.stanford.edu/v1
      timeout: 30
    openai:
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1
    gemini:
      api_key: ${GEMINI_API_KEY}
      base_url: https://generativelanguage.googleapis.com/v1beta
    anthropic:
      api_key: ${ANTHROPIC_API_KEY_o}
      base_url: https://api.anthropic.com
    ollama:
      api_key: ollama  # Default for ollama
      # For framework use
      # base_url: http://doudna:11434
      base_url: http://host.containers.internal:11434 # <_> http://localhost:11434  <_> http://doudna:11434
      # For container services
      # host: doudna # host.containers.internal <_> localhost <_> doudna
      host: host.containers.internal
      port: 11434
